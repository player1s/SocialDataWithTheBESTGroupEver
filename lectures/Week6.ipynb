{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "\n",
    "Phew. Is it week 6 already? \n",
    "\n",
    "Last week we had an intro to machine learning and regression and this week we continue with some more ML but focusing on classification instead. There are lots of courses on machine learning at DTU. And across many research areas, people use ML for all kinds of things. So there's a good chance you're already familiar with what's going to happen today. \n",
    "\n",
    "In the following, we continue introducing fundamentals of ML, decision trees and start with some prediction tasks on crime data. You might ask, why are we doing this? Well, a couple of reasons:\n",
    "\n",
    "1. It ties nicely with how we started this course: do you remember all we learnt about predictive policing in Week 1? So, today it is our turn to make predictions and see how well we can do with the data we have been exploring.\n",
    "\n",
    "2. Visualization **AND** machine learning is a powerful combination. A combination that is pretty rare. \n",
    "  - Usually it's the case that people are either good at machine learning or data viz, but not both. \n",
    "  - So what we will be able to do in this class is an unusual combo: We can use ML to understand data and then visualize the outputs of the machine-learning.\n",
    "    \n",
    "The plan for today is as follows:\n",
    "\n",
    "1. In part 1, we go more in depth on fundamentals of machine learning;\n",
    "2. In part 2, we get an introduction to Decision Trees;\n",
    "3. In part 3, we put everything together to predict criminal recidivism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fundamentals of machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with a couple of lectures from Ole Winter about model selection and feature extraction. These connect nicely with what you should have already read in DSFS Chaper 11. If you did not read the chater yet, it is time for you to do it. \n",
    "\n",
    "Find it on DTU Learn under 'Course content' $\\rightarrow$ 'Content' $\\rightarrow$ 'Lecture 6 reading' \n",
    "\n",
    "**Model selection**\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/MHhlAtw3Ces/0.jpg)](https://www.youtube.com/watch?v=MHhlAtw3Ces)\n",
    "\n",
    "**Feature extraction and selection**\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/RZmitKn220Q/0.jpg)](https://www.youtube.com/watch?v=RZmitKn220Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 1*: A few questions about machine learning to see whether you've read the text and watched the videos. \n",
    ">\n",
    "> * What do we mean by a 'feature' in a machine learning model?\n",
    "\n",
    "<br> \"Features are whatever inputs we provide to our model. + e.g. In the simplest case, features are simply given to you. If you want to predict someone‚Äôs salary based on her years of experience, then years of experience is the only feature you have. (maybe squaring, cubing.. your input features might improve your model performance) + feature can be categorical or numerical -> different models need different input (Naive Bayes classifier [yes-no-features], Regression models [numeric features], decision trees [numeric or categorical features] + might be hard to choose the right amount\" (reading 1 week 6, chapter \"Feature Extraction and Selection\")\n",
    "\n",
    "> * What is the main problem with overfitting?\n",
    "\n",
    "<br> \"A common danger in machine learning is overfitting‚Äîproducing a model that performs well on the data you train it on but that generalizes poorly to any new data. This could involve learning noise in the data. Or it could involve learning to identify\n",
    "specific inputs rather than whatever factors are actually predictive for the desired output (complex models lead to overfitting -> train (often 2/3-s of the data), test and validation split) -> overfitting goof performance on train data, but bad performance on test data (if so good hint, but maybe model still overfitts for the overall problem as there was a common patter in the train and test data + another issue: choosing the right model -> \"split the data into three parts: a training set for building models, a validation set for choosing among trained models, and a test set for judging the final model.).\" (reading 1 week 6, chapter \"Overfitting, Underfitting\")\n",
    "\n",
    "<br> the model complexity should reflect the complexity of the data + overfitting = fitting to the noice of the data (+ little data -> we need to recycle the data e.g. cross validation, nested cross validation)\n",
    "\n",
    "[Overfitting Error Model Complexitiy](https://github.com/Philipp-Otter/socialdata2022/blob/main/files/overfitting_underfitting_error.png)\n",
    "\n",
    "[Data Split](https://github.com/Philipp-Otter/socialdata2022/blob/main/files/data_split.png)\n",
    "\n",
    "[Cross Validation](https://github.com/Philipp-Otter/socialdata2022/blob/main/files/cross_validation.png)\n",
    "\n",
    "\n",
    "> * Explain the connection between the bias-variance trade-off and overfitting/underfitting.\n",
    "\n",
    "<br> \"Both are measures of what would happen if you were to retrain your model many times on different sets of training data (from the same larger population). + For example, the degree 0 model in ‚ÄúOverfitting and Underfitting‚Äù on page 142 will make a lot of mistakes for pretty much any training set (drawn from the same population), which means that it has a high bias. However, any two randomly chosen\n",
    "training sets should give pretty similar models (since any two randomly chosen training sets should have pretty similar average values). So we say that it has a low variance. High bias and low variance typically correspond to underfitting. + On the other hand, the degree 9 model fit the training set perfectly. It has very low bias but very high variance (since any two training sets would likely give rise to very different models). This corresponds to overfitting. + If your model has high bias (which means it performs poorly even on your training data) then one thing to try is adding more features. Going from the degree 0 model in ‚ÄúOverfitting and Underfitting‚Äù on page 142 to the degree 1 model was a big improvement. + If your model has high variance, then you can similarly remove features. But another solution is to obtain more data (if you can). + Holding model complexity constant, the more data you have, the harder it is to overfit. + On the other hand, more data won‚Äôt help with bias. If your model doesn‚Äôt use enough features to capture regularities in the data, throwing more data at it won‚Äôt help.\" (reading 1 week 6, chapter \"The Bias-Variance Trade-off\")\n",
    "\n",
    "> * The `Luke is for leukemia` on page 145 in the reading is a great example of why accuracy is not a good measure in very unbalanced problems. Try to come up with a similar example based on a different type of data (either one you are interested in or one related to the SF crime dataset).\n",
    "\n",
    "<br> a kid named Chris becoming good at soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decision Tree Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to decision trees. This is a fantastically useful supervised machine-learning method, that we use all the time in research. To get started on the decision trees, we asked you to read DSFS, chapter 17 (if you didn't read it you can find it in DTU Learn). \n",
    "\n",
    "And our little session on decision trees wouldn't be complete without hearing from Ole about these things. \n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/LAA_CnkAEx8/0.jpg)](https://www.youtube.com/watch?v=LAA_CnkAEx8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 2:* Just a few questions to make sure you've read the text (DSFS chapter 17) and/or watched the video.\n",
    "> \n",
    "> * There are two main kinds of decision trees depending on the type of output (numeric vs. categorical). What are they?\n",
    "\n",
    "<br> \"Most people divide decision trees into classification trees (which produce categorical outputs) and regression trees (which produce numeric outputs).\" (reading 2 week 6, chapter \"What Is a Decision Tree?\")\n",
    "\n",
    "> * Explain in your own words: Why is entropy useful when deciding where to split the data?\n",
    "\n",
    "<br> \"Ideally, we‚Äôd like to choose questions whose answers give a lot of information about what our tree should predict. If there‚Äôs a single yes/no question for which ‚Äúyes‚Äù answers always correspond to True outputs and ‚Äúno‚Äù answers to False outputs (or vice versa), this would be an awesome question to pick. Conversely, a yes/no question for which neither answer gives you much new information about what the prediction should be is probably not a good choice. We capture this notion of ‚Äúhow much information‚Äù with entropy. You have probably heard this used to mean disorder. We use it to represent the uncertainty associated with data. + Imagine that we have a set S of data, each member of which is labeled as belonging to one of a finite number of classes C1, ..., Cn. If all the data points belong to a single class, then there is no real uncertainty, which means we‚Äôd like there to be low entropy. If the data points are evenly spread across the classes, there is a lot of uncertainty and we‚Äôd like there to be high entropy.\" (reading 2 week 6, chapter \"Entropy\")\n",
    "<br> \"Correspondingly, we‚Äôd like some notion of the entropy that results from partitioning a set of data in a certain way. We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), and high\n",
    "entropy if it contains subsets that (are large and) have high entropy (i.e., are highly uncertain).\" (reading 2 week 6, chapter \"The Entropy of a Partition\")\n",
    "\n",
    "> * Why are trees prone to overfitting?\n",
    "\n",
    "<br> \"One problem with this approach is that partitioning by an attribute with many different values will result in a very low entropy due to overfitting. -> e.g. partitioning on social security numbers will produce one-person subsets with zero entropy -> based on social security number, so cannot be generalised beyond the training set\" (reading 2 week 6, chapter \"The Entropy of a Partition\")\n",
    "\n",
    "\n",
    "> * Explain (in your own words) how random forests help prevent overfitting.\n",
    "\n",
    "<br> \"One way of avoiding overfitting is a technique called random forests, in which we build multiple decision trees and let them vote on how to classify inputs. + Since each tree is built using different data, each tree will be different from every other tree. + A second source of randomness involves changing the way we chose the best_attribute to split on. Rather than looking at all the remaining attributes, we first choose a random subset of them and then split on whichever of those is best.\" (reading 2 week 6, chapter \"Random Forests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following I added some additional material for you to explore decision trees through some fantastic *visual* introductions.** \n",
    "\n",
    "*Decision Trees 1*: The visual introduction to decision trees on this webpage is AMAZING. Take a look to get an intuitive feel for how trees work. Do not miss this one, it's a treat! http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "*Decision Trees 2*: the second part of the visual introduction is about the topic of model selection, and bias/variance tradeoffs that we looked into earlier during this lesson. But once again, here those topics are visualized in a fantastic and inspiring way, that will make it stick in your brain better. So check it out http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n",
    "\n",
    "\n",
    "\n",
    "*Decision tree tutorials*: And of course the best way to learn how to get this stuff rolling in practice, is to work through a tutorial or two. We recommend the ones below:\n",
    "  * https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "  * https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 (this one also has good considerations regarding the one-hot encodings)\n",
    "  \n",
    "(But there are many other good ones out there.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predicting criminal recidivism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to put everything together and use the models we have read about for prediction. Today, we are still going to focus on crimes, but with a different dataset. \n",
    "\n",
    "The dataset is related to an algorithm used by judges and parole officers for scoring criminal defendant‚Äôs likelihood of reoffending (recidivism). It consists of information about defendants and variables used to measure recidivism. \n",
    "\n",
    "I'll provide you with more information about this data and its source next week. But, for now I don't want to give you more spoilers (you'll know why next week üòá), so let's get started. In the next exercises, we will try to **loosely** recreate the algorithm to predict whether a person is going to re-commit a crime in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 3.1:* Getting the data ready. Before getting to predictions, we need to get the data, select the features, and define the target. Follow these steps for success:\n",
    ">\n",
    "> * Download the dataset from [GitHub](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/recidivism_dataset_sub.csv) and load it in a `pandas` dataframe.\n",
    "> * Select the variables of interest. Here, a description of which one and their meaning:\n",
    ">    1. `age`: age (in years) of the person,;\n",
    ">    2. `sex`: either \"Female\" or \"Male\";\n",
    ">    3. `race`: a variable encoding the race of the person;\n",
    ">    4. `juv_fel_count`: the number of previous juvenile felonies;\n",
    ">    5. `juv_misd_count`: the number of previous juvenile misdemeanors;\n",
    ">    6. `juv_other_count`: the number of prior juvenile convictions that are not considered either felonies or misdemeanors;\n",
    ">    7. `priors_count`: the number of prior crimes committed;\n",
    ">    8. `is_recid`: if the defendent has recommit a crime;\n",
    ">    9. `days_b_screening_arrest`: Days between the arrest and screening.\n",
    ">    9. `c_charge_degree`: Degree of the crime. It is either M (Misdemeanor), F (Felony), or O (not causing jail)\n",
    ">\n",
    "> * Finally, we need a target:\n",
    ">    * `two_year_recid` is what we want to predict. Its current values are $\\in\\left[0,1\\right]$, where $0$ means the defendant did not recommit a crime within two years, and $1$ means the defendant recommitted a crime within two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to see all columns when e.g. calling the head function\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidivism = pd.read_csv('https://raw.githubusercontent.com/suneman/socialdata2022/main/files/recidivism_dataset_sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we now have the data, but we still need a bit of **preprocessing** before we can get to the actual prediction.\n",
    "\n",
    "At the beginning, I wanted you to embed everything into a unique pipeline. I later found that it sometimes have issues (throw errors, takes long time when cross-validating, etc.). Thus, I have excluded this step from today's class. However, if you want to know more about pipelines, here, a nice optional tutorial for you:\n",
    "\n",
    "* https://towardsdatascience.com/step-by-step-tutorial-of-sci-kit-learn-pipeline-62402d5629b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 3.2:* Data preprocessing and label encoding. \n",
    ">\n",
    "> * To preprocess the data follow these steps:\n",
    ">    * filter out records where the `is_recid` feature is not known (i.e. where it is equal to -1); \n",
    ">    * only keep records that cause jail time;\n",
    ">    * only keep records that have between $-30$ and $30$ days between the arrest and screening.\n",
    ">    * Finally, drop `is_recid`, `c_charge_degree`, `days_b_screening_arrest` for the upcoming analysis.\n",
    "> * Before we move on, let's explore the data with a few visualizations. Use the variable `two_year_recid` and create a plot with the following subplots: \n",
    ">     * A bar plot with the number of recommitted and non-recommitted crimes, e.g., number of 0s and 1s in `two_year_recid`. Now a couple of questions: What is the fraction of recommitted crimes over the total number of records? Is it balanced?\n",
    ">     * A bar plot with the fraction of recommitted crimes over total number of records per `sex`, e.g., the number of Females that recommitted a crime over the number of all female records. What do you observe? \n",
    ">     * A bar plot with the fraction of recommitted crimes over total number of records per `race` (compute as above). What do you observe?\n",
    ">     * A bar plot with the fraction of recommitted crimes over total number of records per `age` group (group ages as <20, 20-30, 30-40, etc. and compute as above). What do you observe?\n",
    "> * Some features we are working with are categorical, so we need to deal with them by using encoders. There are many different types, but we will focus on the `OneHotEncoder` and the `LabelEncoder`:\n",
    ">    * Describe what these encoder do and choose one. Which one did you choose? Why?\n",
    ">    * What variables need to be transformed? \n",
    "\n",
    "<mark> **Note** The data source that I was using has changed, so the data currently doesn't include `is_recid=-1`and `c_charge_degree='O'`. Please, write the code as if you were filtering those variables anyway, it is a way for you to practice with `pandas`.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>c_case_number</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>c_arrest_date</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>c_charge_desc</th>\n",
       "      <th>is_recid</th>\n",
       "      <th>r_case_number</th>\n",
       "      <th>r_charge_degree</th>\n",
       "      <th>r_days_from_arrest</th>\n",
       "      <th>r_offense_date</th>\n",
       "      <th>r_charge_desc</th>\n",
       "      <th>r_jail_in</th>\n",
       "      <th>r_jail_out</th>\n",
       "      <th>violent_recid</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>vr_case_number</th>\n",
       "      <th>vr_charge_degree</th>\n",
       "      <th>vr_offense_date</th>\n",
       "      <th>vr_charge_desc</th>\n",
       "      <th>type_of_assessment</th>\n",
       "      <th>decile_score.1</th>\n",
       "      <th>score_text</th>\n",
       "      <th>screening_date</th>\n",
       "      <th>v_type_of_assessment</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-08-13 06:03:42</td>\n",
       "      <td>2013-08-14 05:41:20</td>\n",
       "      <td>13011352CF10A</td>\n",
       "      <td>2013-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Aggravated Assault w/Firearm</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-01-26 03:45:27</td>\n",
       "      <td>2013-02-05 05:36:53</td>\n",
       "      <td>13001275CF10A</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Felony Battery w/Prior Convict</td>\n",
       "      <td>1</td>\n",
       "      <td>13009779CF10A</td>\n",
       "      <td>(F3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>Felony Battery (Dom Strang)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>13009779CF10A</td>\n",
       "      <td>(F3)</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>Felony Battery (Dom Strang)</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-04-13 04:58:34</td>\n",
       "      <td>2013-04-14 07:02:04</td>\n",
       "      <td>13005330CF10A</td>\n",
       "      <td>2013-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession of Cocaine</td>\n",
       "      <td>1</td>\n",
       "      <td>13011511MM10A</td>\n",
       "      <td>(M1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>Driving Under The Influence</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000570CF10A</td>\n",
       "      <td>2013-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession of Cannabis</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>8</td>\n",
       "      <td>High</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12014130CF10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>76.0</td>\n",
       "      <td>F</td>\n",
       "      <td>arrest case no charge</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id compas_screening_date   sex  age          age_cat              race  juv_fel_count  decile_score  juv_misd_count  juv_other_count  priors_count  days_b_screening_arrest            c_jail_in           c_jail_out  c_case_number c_offense_date c_arrest_date  c_days_from_compas c_charge_degree                   c_charge_desc  is_recid  r_case_number r_charge_degree  r_days_from_arrest r_offense_date                r_charge_desc   r_jail_in  r_jail_out  violent_recid  is_violent_recid vr_case_number vr_charge_degree vr_offense_date               vr_charge_desc  type_of_assessment  decile_score.1 score_text screening_date v_type_of_assessment  v_decile_score v_score_text v_screening_date  in_custody out_custody  priors_count.1  start   end  event  two_year_recid\n",
       "0   1            2013-08-14  Male   69  Greater than 45             Other              0             1               0                0             0                     -1.0  2013-08-13 06:03:42  2013-08-14 05:41:20  13011352CF10A     2013-08-13           NaN                 1.0               F    Aggravated Assault w/Firearm         0            NaN             NaN                 NaN            NaN                          NaN         NaN         NaN            NaN                 0            NaN              NaN             NaN                          NaN  Risk of Recidivism               1        Low     2013-08-14     Risk of Violence               1          Low       2013-08-14  2014-07-07  2014-07-14               0      0   327      0               0\n",
       "1   3            2013-01-27  Male   34          25 - 45  African-American              0             3               0                0             0                     -1.0  2013-01-26 03:45:27  2013-02-05 05:36:53  13001275CF10A     2013-01-26           NaN                 1.0               F  Felony Battery w/Prior Convict         1  13009779CF10A            (F3)                 NaN     2013-07-05  Felony Battery (Dom Strang)         NaN         NaN            NaN                 1  13009779CF10A             (F3)      2013-07-05  Felony Battery (Dom Strang)  Risk of Recidivism               3        Low     2013-01-27     Risk of Violence               1          Low       2013-01-27  2013-01-26  2013-02-05               0      9   159      1               1\n",
       "2   4            2013-04-14  Male   24     Less than 25  African-American              0             4               0                1             4                     -1.0  2013-04-13 04:58:34  2013-04-14 07:02:04  13005330CF10A     2013-04-13           NaN                 1.0               F           Possession of Cocaine         1  13011511MM10A            (M1)                 0.0     2013-06-16  Driving Under The Influence  2013-06-16  2013-06-16            NaN                 0            NaN              NaN             NaN                          NaN  Risk of Recidivism               4        Low     2013-04-14     Risk of Violence               3          Low       2013-04-14  2013-06-16  2013-06-16               4      0    63      0               1\n",
       "3   5            2013-01-13  Male   23     Less than 25  African-American              0             8               1                0             1                      NaN                  NaN                  NaN  13000570CF10A     2013-01-12           NaN                 1.0               F          Possession of Cannabis         0            NaN             NaN                 NaN            NaN                          NaN         NaN         NaN            NaN                 0            NaN              NaN             NaN                          NaN  Risk of Recidivism               8       High     2013-01-13     Risk of Violence               6       Medium       2013-01-13         NaN         NaN               1      0  1174      0               0\n",
       "4   6            2013-03-26  Male   43          25 - 45             Other              0             1               0                0             2                      NaN                  NaN                  NaN  12014130CF10A            NaN    2013-01-09                76.0               F           arrest case no charge         0            NaN             NaN                 NaN            NaN                          NaN         NaN         NaN            NaN                 0            NaN              NaN             NaN                          NaN  Risk of Recidivism               1        Low     2013-03-26     Risk of Violence               1          Low       2013-03-26         NaN         NaN               2      0  1102      0               0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "# 1. filter out records where the `is_recid` feature is not known (i.e. where it is equal to -1)\n",
    "\n",
    "recidivism = recidivism.drop(recidivism[recidivism.is_recid == -1].index)\n",
    "\n",
    "# 2. only keep records that cause jail time\n",
    "\n",
    "recidivism.head()\n",
    "\n",
    "# 3. only keep records that have between $-30$ and $30$ days between the arrest and screening\n",
    "\n",
    "\n",
    "\n",
    "# 4. drop `is_recid`, `c_charge_degree`, `days_b_screening_arrest` for the upcoming analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost there! It is now time to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 3.3:* Build a Decision Tree or a Random Forest. Now we are going to build a Decision Tree (or a Random Forest) classifier that takes as input the features defined above and predicts if a person is going to recommit the crime within two years.\n",
    "> * Split the data in Train/Test sets. You can do this with `train_test_split` in `sklearn`, I used a 70/30 split, but you are free to try different ones. \n",
    ">     * **Note:** create a balanced dataset, that is, **grab an equal number of examples** from each target value.\n",
    ">    * Fit a model to your Train set. A good option is the  `DecisionTreeClassifier` (or even better a [Random Forest](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html), here is [another tutorial for Random Forests](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0)).\n",
    "> * Evaluate the performance of model on the test set (look at Accuracy, Precision, and Recall). What are your thoughts on these metrics? Is accuracy a good measure?\n",
    ">    * **hint:** Since you have created a balanced dataset, the baseline performance (random guess) is 50%. \n",
    "> * Are your results tied to the specific training data/hyperparameter set you used? Try to perform a `RandomizedSearchCV` and recompute the performance metric above with the hyperparameters found. [Here](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) a nice tutorial for you! And here one on [cross-validation](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) for those of you who crave for more.\n",
    "> * Visualize the tree. There are different options to do so. The easiest one is to use `plot_tree`, but there are other [options](https://mljar.com/blog/visualize-decision-tree/). If you chose Random Forest, you can visualize a tree as well by extracting a single tree with `model.estimators_[n]` (n is the index of the estimator you want to select).\n",
    "> * Visualize the Feature Importance. What do you observe?\n",
    "> * **(Optional)** If you find yourself with extra time, come back to this exercise and tweak the encoder, model, and variables you use to see if you can improve the performance of the tree. **Note**: It's not 100% given that adding variables will improve your predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you go, please, have a look at the following two activities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1)\n",
    "\n",
    "<mark> Take a minute (it is really one minute) to fill this [form](https://forms.gle/9RwhFc96na4E2Fmg7). It is really important for me to continue improving and give you better feedbacks. </mark>\n",
    "\n",
    "---\n",
    "2)\n",
    "\n",
    "<mark> Some of you consider this course too easy. So, it's time to spice things up: once you have the best model you could find, go to DTU Learn and submit your code together with your final accuracy/precision/recall scores under DTU-Learn $\\rightarrow$ Assignments. I'll make a Leaderboard and we'll see who's gonna win ü•á!!</mark>\n",
    "\n",
    "**Constraints:** Use a 70/30 train/test split, and `random_seed=42`.\n",
    "\n",
    "**Note 1:** Even if it is in the form of an assignment on DTU Learn it is **not** going to be evaluated. So, take it really as an opportunity to play around with your model and see how well you can do.\n",
    "\n",
    "**Note 2:** You have time until **Thursday at 23.59** to submit your model/performance score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
